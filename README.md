# 项目目标

本项目旨在提升模型的英翻中能力，通过构造偏好数据集并进行模型微调来实现。具体目标如下：
- 收集5000条英文语料，基于不同的模型和不同的提示词，分别生成直译和意译结果，构造偏好数据集。
- 使用`DPO`（Direct Preference Optimization）或`ORPO`（Odds Ratio Preference Optimization）对模型进行微调。
- 提升模型的英翻中能力，特别是在针对复杂领域语料的翻译表现。

# 数据源

本项目的数据源来自以下三个渠道：

1. **arXiv**:
   - 使用arXiv的Python API获取计算机科学领域的论文摘要。
   - 使用`GPT-4o`随机生成30个相关话题，每个话题获取200篇论文，最终共计6000篇论文的摘要。
   - **注意**：由于一次请求过多的论文会导致中断，采用分批次请求方式，每次请求较少的论文以避免中断。

2. **Wikipedia**:
   - 从[OpenDataLab 引领AI大模型时代的开放数据平台](https://opendatalab.com/ABear/Wiki_EN/explore/main)上下载了Wiki_EN的数据集，数据大小为14.1GB。数据格式为jsonl, 字段为：id， source_id，doc_id，data_type，data_source，data_url，content，remark
   - content中的数据标题和段落之间使用`\n`分割，`##`和`###`分别表示一级和二级标题。
   - 翻译语料主要是选择词数在 200 左右的段落。并且会使用nltk包+一些规则进行句子完整性检查并GPT-4o mini进行语法修正。

3. **The Economist**:
   - 下载了2022年50多篇PDF文件。
   - 在GPU机器上部署了`MinerU`服务进行PDF解析，每篇解析耗时约15分钟。
   - 解析后的英文文本存在部分语法问题，后续通过`GPT-4o mini`进行语法修正。

# 数据处理流程

1. **数据收集**：
   - 从arXiv获取1000个摘要，从Wikipedia获取3000个片段，从The Economist获取1000个片段，合计5000条数据。

2. **直译生成**：
   - 基于`ollama`在本地部署的`Qwen2-7B-Instruct`模型，对上述英文片段生成直译结果。

3. **意译生成**：
   - 使用`OpenRouter`提供的`GPT-4o mini`接口，对直译结果进行两轮反思迭代，思考并改进翻译质量，最终生成意译结果。

4. **高难度数据处理**：
   - 由于arXiv摘要难度较高，采用了更强大的模型进行处理。使用`Poe`平台的`GPT-4o`模型，并结合录屏精灵进行自动化处理，平均每40秒处理一条数据，处理结果保存至`Notion`。

# 文件说明

- **literal_translation.py**: 
  - 该脚本处理英文原文数据，并调用`Qwen2-7B-Instruct`模型生成直译结果。

- **idiomatic_translation.py**: 
  - 该脚本处理英文原文和直译结果，调用`GPT-4o mini`模型，使用两次反思迭代技术生成意译结果。

- **语法纠错提示词.md**:
  - 包含用于`GPT-4o mini`进行语法修正的提示词，主要用于修复`MinerU`解析后的`The Economist`英文片段中存在的语法混乱问题。

- **意译提示词.md**:
  - 包含用于意译生成的提示词，结合两次迭代反思技巧，进一步优化直译结果。

- **翻译质量评分提示词**:
  - 提供用于评估翻译质量的提示词，基于`LLM-as-a-Judge`思想，让大模型对翻译质量进行评价。

# 注意事项

- **arXiv请求限制**: 
  - 一次请求过多的论文摘要可能会导致请求中断，因此采用了分批次请求的方式，每次请求较少数量的论文，确保请求稳定性。
  
- **模型选择**: 
  - 对于复杂的arXiv摘要，使用了更强大的模型（如`Poe`平台上的`GPT-4o`模型）进行处理，以确保翻译质量。对于其他较简单的文本，则使用较弱的模型(`OpenRouter的GPT-4o mini`)生成直译和意译结果。